{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bee11-44b2-40ad-a330-a6efe8509b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f031d4e5-7cba-4912-8822-bdbff335986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS- Missing values in a dataset refer to the absence of a value for a particular feature or attribute of an observation. \n",
    "     Missing values can occur for various reasons, such as human error, data corruption, or data processing issues.\n",
    "\n",
    "It is essential to handle missing values in a dataset because they can cause bias in the analysis and modeling results. \n",
    "Missing values can reduce the sample size, which can lead to inaccurate estimates and conclusions. \n",
    "Moreover, missing values can also affect the relationships between variables and lead to incorrect predictions.\n",
    "\n",
    "Some algorithms that are not affected by missing values are:\n",
    "\n",
    "1. Tree-based algorithms such as decision trees and random forests: These algorithms do not require imputation of missing values as they can handle \n",
    "                                                                    missing values in a feature by using surrogate splits.\n",
    "\n",
    "2. Support Vector Machines (SVMs): SVMs are not affected by missing values as they use a subset of the training data, known as support vectors, \n",
    "                                   to build the model.\n",
    "\n",
    "3. K-Nearest Neighbors (KNN): KNN is also not affected by missing values as it uses the nearest neighbors to make predictions, and missing values \n",
    "                              are automatically handled by the algorithm.\n",
    "\n",
    "However, most machine learning algorithms, such as linear regression, logistic regression, and neural networks, require complete data to work \n",
    "correctly. \n",
    "Therefore, it is essential to handle missing values before applying these algorithms to the dataset. \n",
    "There are several methods to handle missing values, such as imputation, deletion, and prediction-based imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b8e00-4927-4dbf-ba4b-dd4c33ed9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad81ba2-cfe2-4a47-90db-485f14e416b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS- There are several techniques used to handle missing data, and some of them are:\n",
    "\n",
    "1. Deletion: In this technique, the missing values are deleted from the dataset. \n",
    "             There are two methods of deletion, listwise deletion, and pairwise deletion.\n",
    "    \n",
    "    import pandas as pd\n",
    "\n",
    "    # Create a DataFrame with missing values\n",
    "    df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "    # Listwise deletion\n",
    "    df1 = df.dropna()\n",
    "\n",
    "    # Pairwise deletion\n",
    "    df2 = df.dropna(axis=1, how='all')\n",
    "\n",
    "2. Imputation: In this technique, missing values are replaced with some estimate or imputed values.\n",
    "Example:\n",
    "    \n",
    "    # Imputation using Mean\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    # Imputation using Median\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "\n",
    "    # Imputation using Mode\n",
    "    df.fillna(df.mode(), inplace=True)\n",
    "\n",
    "    # Imputation using Interpolation\n",
    "    df.interpolate(method='linear', inplace=True)\n",
    "\n",
    "3. Prediction-based methods: In this technique, machine learning models are used to predict the missing values based on other features.\n",
    "Example:\n",
    "    \n",
    "    # Predictive Imputation using KNN \n",
    "    from sklearn.impute import KNNImputer\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "4. Special value imputation: In this technique, missing values are replaced by special values such as \"Unknown\" or \"-999\".\n",
    "Example:\n",
    "    \n",
    "    # Special value imputation\n",
    "     df.fillna('Unknown', inplace=True)\n",
    "        \n",
    "        \n",
    "These are some of the techniques used to handle missing data in a dataset. The choice of the technique depends on the characteristics of the data, \n",
    "the amount of missing data, and the objective of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8afed03-d3df-4021-b450-dc8a9b791c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd812d45-a9f9-4be2-9c28-2c4b0c3e84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS- Imbalanced data refers to a situation where the number of observations in one class or category is significantly higher than the number of \n",
    "     observations in another class or category. In other words, the data is not evenly distributed among the classes, and one class has a much \n",
    "     larger representation than the other.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to biased machine learning models and incorrect predictions. The models trained on imbalanced data \n",
    "tend to be biased towards the majority class, as they are trained to optimize the overall accuracy rather than the accuracy of individual classes. \n",
    "As a result, the minority class is often misclassified, leading to a low recall or true positive rate.\n",
    "\n",
    "For example, suppose we have a dataset with 100 observations, out of which 90 belong to class A, and 10 belong to class B. \n",
    "If we train a machine learning model on this dataset without handling the imbalance, it is likely to predict all observations as class A, \n",
    "as it will optimize for overall accuracy. In this case, the model will have a high accuracy of 90%, but a very low recall of 0% for class B, \n",
    "which means that it will not correctly identify any observation from class B.\n",
    "\n",
    "To avoid these issues, it is essential to handle imbalanced data by using techniques such as oversampling, undersampling, and synthetic sampling. \n",
    "These techniques balance the distribution of classes and help the machine learning models to learn equally from all classes, leading to better \n",
    "predictions and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b889142-1048-4908-bce6-9f02393c6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8728c89d-bfaf-46ec-b209-23f4e8c1dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS- Up-sampling and down-sampling are two techniques used to handle imbalanced data in machine learning.\n",
    "\n",
    "Down-sampling involves reducing the number of observations in the majority class to make it more balanced with the minority class. \n",
    "This can be done randomly or by selecting a subset of observations that are representative of the majority class. \n",
    "For example, if we have a dataset with 1000 observations, out of which 900 belong to class A and 100 belong to class B, \n",
    "we can down-sample class A to 100 observations to balance the distribution.\n",
    "\n",
    "Up-sampling, on the other hand, involves increasing the number of observations in the minority class to make it more balanced with the majority class. \n",
    "This can be done by replicating the minority class observations or by generating synthetic observations using techniques such as \n",
    "SMOTE (Synthetic Minority Over-sampling Technique). \n",
    "For example, if we have a dataset with 1000 observations, out of which 900 belong to class A and 100 belong to class B, \n",
    "we can up-sample class B to 900 observations to balance the distribution.\n",
    "\n",
    "\n",
    "When to use up-sampling and down-sampling:\n",
    "\n",
    "Down-sampling is generally used when the majority class has a significantly higher number of observations than the minority class, \n",
    "and the dataset is large enough to provide representative samples of both classes even after reducing the size of the majority class.\n",
    "\n",
    "Up-sampling is generally used when the minority class has a relatively small number of observations, and replicating them is not sufficient \n",
    "to balance the distribution. It can also be used when the cost of misclassifying the minority class is high, and it is essential to improve the \n",
    "model's performance on the minority class.\n",
    "\n",
    "In summary, up-sampling and down-sampling are techniques used to balance the distribution of classes in an imbalanced dataset, and \n",
    "the choice of technique depends on the characteristics of the data and the objective of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c268cc92-20f6-478e-ac1b-71828c708dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d70cb4-6042-4906-875f-b6fb007e56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS- Data augmentation is a technique used to increase the size of a dataset by generating new data samples from the existing ones, \n",
    "     while preserving the original distribution and structure of the data. The goal of data augmentation is to improve the performance of \n",
    "     machine learning models by increasing the amount and diversity of training data, without collecting new data.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation technique used to address the issue of imbalanced datasets, \n",
    "where the minority class has significantly fewer observations than the majority class. \n",
    "SMOTE generates synthetic observations for the minority class by interpolating between existing observations.\n",
    "\n",
    "Here's how SMOTE works:\n",
    "\n",
    "1. Select a random observation from the minority class.\n",
    "\n",
    "2. Find the k nearest neighbors of the selected observation in the feature space.\n",
    "\n",
    "3. Select one of the k neighbors at random and generate a new observation by interpolating between the selected observation and the randomly \n",
    "   selected neighbor. The interpolation is performed in each feature dimension independently by taking a weighted average of the values of the \n",
    "    selected observation and the neighbor.\n",
    "\n",
    "Repeat steps 1 to 3 until the desired number of synthetic observations is generated.\n",
    "\n",
    "The interpolation process of SMOTE generates new data points that lie on the line connecting the selected observation and its neighbors in the \n",
    "feature space, which increases the diversity of the minority class without introducing bias. This helps to improve the performance of \n",
    "machine learning models on the minority class and reduce the risk of overfitting to the majority class.\n",
    "\n",
    "In summary, data augmentation techniques such as SMOTE are used to increase the size and diversity of datasets, which can help to improve the \n",
    "performance of machine learning models, particularly on imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db2297-b19b-47d7-becf-747d96884cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df410ccb-4cdf-43b8-9191-da01f148a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS- Outliers are data points that deviate significantly from the rest of the data in a dataset. These data points are either much larger or \n",
    "     much smaller than the majority of the data points in the dataset, and they can have a significant impact on the results of statistical analyses \n",
    "     and machine learning models.\n",
    "\n",
    "It is essential to handle outliers because they can skew the results of statistical analyses and machine learning models, and lead to inaccurate \n",
    "conclusions and predictions. Outliers can also affect the performance of algorithms that are sensitive to the range and distribution of the data, \n",
    "such as linear regression, K-means clustering, and PCA (Principal Component Analysis).\n",
    "\n",
    "Handling outliers involves identifying and either removing or modifying the outlier data points. The method used to handle outliers depends on the \n",
    "nature and context of the data, and the specific objective of the analysis or model.\n",
    "\n",
    "Some common techniques for handling outliers include:\n",
    "\n",
    "1. Removing outliers: This involves removing the outlier data points from the dataset. However, this can lead to a loss of information and \n",
    "                      reduced sample size, which can affect the statistical power and generalizability of the results.\n",
    "\n",
    "2. Winsorizing: This involves replacing the outlier values with the nearest non-outlier values in the dataset. This can help to preserve the \n",
    "                distribution of the data while reducing the impact of outliers.\n",
    "\n",
    "3. Transformation: This involves applying a mathematical transformation to the data, such as taking the logarithm or square root of the data, \n",
    "                   to reduce the influence of outliers.\n",
    "\n",
    "4. Using robust statistical methods: This involves using statistical methods that are less sensitive to outliers, such as median instead of mean or \n",
    "                                     non-parametric statistical tests.\n",
    "\n",
    "In summary, handling outliers is essential to ensure the accuracy and reliability of statistical analyses and machine learning models, \n",
    "and there are various techniques available to handle outliers, depending on the context and objective of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311c1b9-6029-40af-a545-e82c6b4bb920",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some \n",
    "   techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43336c2f-ab4e-48bf-90fb-ff3382e61b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS- Handling missing data is an important step in data analysis to ensure accurate and reliable results. There are several techniques that can be \n",
    "     used to handle missing data, including:\n",
    "\n",
    "1. Deletion: This involves deleting the rows or columns that contain missing data. This method can be used when the amount of missing data is \n",
    "             relatively small, and it does not significantly affect the analysis. However, this method can lead to a loss of information and \n",
    "             reduced sample size.\n",
    "\n",
    "2. Imputation: This involves replacing the missing data with estimated values based on the observed data. There are several methods for imputation, \n",
    "including:\n",
    "\n",
    "a. Mean, Median or Mode imputation: This involves replacing the missing data with the mean, median, or mode of the observed data for that variable. \n",
    "                                    This method is simple and can work well when the data is normally distributed, but it can lead to biased estimates \n",
    "                                    when the data is skewed.\n",
    "\n",
    "b. Regression imputation: This involves using a regression model to estimate the missing values based on the observed data. This method can work \n",
    "                          well when there are strong relationships between the missing variable and other variables in the dataset.\n",
    "\n",
    "c. Multiple imputation: This involves creating multiple imputed datasets based on different plausible values for the missing data and combining the \n",
    "                        results to obtain a final estimate. This method can help to preserve the uncertainty of the estimates and reduce bias.\n",
    "\n",
    "3. Prediction modeling: This involves using machine learning algorithms such as random forests or gradient boosting to predict the missing values \n",
    "                        based on the observed data. This method can be particularly useful when there are complex relationships between the variables \n",
    "                        in the dataset.\n",
    "\n",
    "In summary, there are several techniques available to handle missing data, and the choice of method depends on the nature and context of the data, \n",
    "and the specific objective of the analysis. It is important to carefully consider the implications of missing data and choose an appropriate method \n",
    "to ensure accurate and reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa04a4a1-6cc9-4397-bc50-906fcb8a2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine \n",
    "   if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6501be3-ac2b-4a79-a7e9-ad42689e0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS- When dealing with missing data, it is important to determine if the missing data is missing at random (MAR) or if there is a pattern to the \n",
    "     missing data, as this can impact the validity of the analysis and the choice of imputation method. \n",
    "    \n",
    "Here are some strategies to determine if the missing data is MAR or not:\n",
    "\n",
    "1. Analyze patterns of missingness: Examine the pattern of missingness in the data to determine if there are any systematic patterns or \n",
    "                                    dependencies between missingness and other variables in the dataset. \n",
    "    For example, if missingness is more prevalent in certain subgroups of the data or for certain variables, this may suggest non-random missingness.\n",
    "\n",
    "2. Use statistical tests: Perform statistical tests to determine if there is a significant association between missingness and other variables in the \n",
    "                          dataset. \n",
    "    For example, chi-squared tests can be used to determine if missingness is associated with categorical variables, and t-tests or ANOVA can be used for continuous variables.\n",
    "\n",
    "3. Visualize the missing data: Create visualizations, such as heatmaps or scatterplots, to visualize the distribution of missing data across the \n",
    "                               dataset and identify any patterns or correlations.\n",
    "\n",
    "4. Use imputation methods: Use different imputation methods to impute the missing data and compare the results to determine if the missing data \n",
    "                           has a significant impact on the analysis or model performance. \n",
    "    For example, if the results are sensitive to the choice of imputation method, this may suggest non-random missingness.\n",
    "\n",
    "In summary, by analyzing patterns of missingness, performing statistical tests, visualizing the missing data, and comparing different \n",
    "imputation methods, it is possible to determine if the missing data is MAR or if there is a pattern to the missing data. \n",
    "This information can guide the choice of imputation method and help ensure the validity of the analysis or model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ecc428-d656-4dfd-ac1e-4d4fcc828928",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, \n",
    "   while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this \n",
    "    imbalanced dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9351f6-c9ac-43b5-b020-4f747b259629",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS- When dealing with imbalanced datasets, such as in a medical diagnosis project where the majority of patients do not have the \n",
    "     condition of interest, it is important to use evaluation metrics that take into account the class imbalance. \n",
    "\n",
    "Here are some strategies that can be used to evaluate the performance of a machine learning model on an imbalanced dataset:\n",
    "\n",
    "1. Confusion matrix: Use a confusion matrix to evaluate the number of true positives, true negatives, false positives, and false negatives. \n",
    "                     This can be used to calculate evaluation metrics such as sensitivity, specificity, precision, and recall.\n",
    "\n",
    "2. ROC curve: Plot the receiver operating characteristic (ROC) curve, which shows the true positive rate (sensitivity) against the \n",
    "              false positive rate (1-specificity) at various threshold values. The area under the ROC curve (AUC) can be used as an evaluation metric, \n",
    "              with higher values indicating better performance.\n",
    "\n",
    "3. Precision-recall curve: Plot the precision-recall curve, which shows the precision (positive predictive value) against the recall (sensitivity) at \n",
    "                           various threshold values. The area under the precision-recall curve (AUPRC) can be used as an evaluation metric, \n",
    "                           with higher values indicating better performance.\n",
    "\n",
    "4. Stratified sampling: Use stratified sampling to ensure that the training and test sets have a similar proportion of samples from each class. \n",
    "                        This can help prevent bias towards the majority class and improve the generalization performance of the model.\n",
    "\n",
    "5. Class weighting: Use class weighting to adjust the weights of the different classes during training. This can give more weight to the minority \n",
    "                    class and help prevent the model from being biased towards the majority class.\n",
    "\n",
    "6. Resampling techniques: Use resampling techniques such as oversampling or undersampling to balance the classes in the training set. \n",
    "   For example, synthetic minority over-sampling technique (SMOTE) can be used to generate synthetic samples of the minority class to increase the \n",
    "    size of the minority class.\n",
    "\n",
    "In summary, by using evaluation metrics that take into account the class imbalance, using stratified sampling, class weighting, and \n",
    "resampling techniques, it is possible to evaluate the performance of a machine learning model on an imbalanced dataset and \n",
    "improve its generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2507670-7130-4133-a990-5f0a15fbe226",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers \n",
    "    reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd336f81-7b49-4e6f-b14b-4d44e6f22b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS- When dealing with an unbalanced dataset in a project that aims to estimate customer satisfaction, there are several methods that can be used \n",
    "     to balance the dataset and down-sample the majority class. \n",
    "    \n",
    "Here are a few techniques that can be used:\n",
    "\n",
    "1. Random under-sampling: Randomly select a subset of the majority class samples to create a more balanced dataset. This method can be simple \n",
    "                          but may result in the loss of valuable data.\n",
    "\n",
    "2. Cluster-based under-sampling: Use clustering algorithms to group similar majority class samples and select representative samples to \n",
    "                                 keep in the dataset.\n",
    "\n",
    "3. Tomek links: Use the Tomek links algorithm to remove samples that are near the decision boundary between the minority and majority classes.\n",
    "\n",
    "4. SMOTE: Synthetic Minority Over-sampling Technique (SMOTE) is a popular method for balancing datasets by generating synthetic samples \n",
    "          of the minority class. It creates new samples by interpolating between existing minority class samples and their nearest neighbors.\n",
    "\n",
    "To down-sample the majority class, random under-sampling and cluster-based under-sampling can be used. \n",
    "For example, the imbalanced-learn library in Python provides various under-sampling methods, including RandomUnderSampler and ClusterCentroids.\n",
    "\n",
    "Here is an example of using RandomUnderSampler to down-sample the majority class:\n",
    "    \n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "    X, y = load_customer_data()\n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "This code uses RandomUnderSampler to randomly select a subset of the majority class samples and returns a new balanced dataset with X_resampled \n",
    "and y_resampled. Similar methods can be used for other under-sampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75328caf-8426-42aa-9d23-b3b8434663d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the \n",
    "    occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ed711-fc00-47cc-a7b1-5619d8186868",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS- When dealing with a dataset that is unbalanced with a low percentage of occurrences of a rare event, there are several methods that can be\n",
    "     employed to balance the dataset and up-sample the minority class. \n",
    "\n",
    "Here are a few techniques that can be used:\n",
    "\n",
    "1. Random over-sampling: Randomly duplicate the minority class samples to create a more balanced dataset. This method can be simple but \n",
    "                         may result in overfitting and reduced generalization.\n",
    "\n",
    "2. SMOTE: Synthetic Minority Over-sampling Technique (SMOTE) is a popular method for balancing datasets by generating synthetic samples of the \n",
    "          minority class. It creates new samples by interpolating between existing minority class samples and their nearest neighbors.\n",
    "\n",
    "3. ADASYN: Adaptive Synthetic Sampling (ADASYN) is another method for generating synthetic samples of the minority class, which focuses on creating \n",
    "           samples in regions that are harder to learn by the classifier.\n",
    "\n",
    "4. SMOTE-ENN: SMOTE combined with Edited Nearest Neighbors (SMOTE-ENN) is a hybrid method that first uses SMOTE to oversample the minority class and \n",
    "              then uses ENN to remove any noisy samples.\n",
    "\n",
    "To up-sample the minority class, SMOTE, ADASYN, and SMOTE-ENN can be used. Here is an example of using SMOTE to up-sample the minority class:\n",
    "    \n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    X, y = load_rare_event_data()\n",
    "    smote = SMOTE(random_state=0)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "This code uses SMOTE to generate new synthetic samples of the minority class and returns a new balanced dataset with X_resampled and y_resampled. \n",
    "Similar methods can be used for other up-sampling techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
